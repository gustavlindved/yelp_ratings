{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Yelp_ordinal_approach.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ge38EN4NEjE5",
        "colab_type": "code",
        "outputId": "3412aed2-8902-4540-eef5-1634381d77c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "# Import packages\n",
        "import os, re, string\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torchtext import data\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from torchtext.vocab import GloVe\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn as nn\n",
        "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ocp-ZUwHDWS2",
        "colab_type": "code",
        "outputId": "1aaa4f0b-55d8-4964-c9a0-2f63b903e09c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "# Mount google drive to colab to import data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "os.chdir('gdrive/My Drive/Colab Notebooks/Yelp')\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "'Copy of One_hot_yelp.ipynb'\n",
            " One_hot_yelp.ipynb\n",
            " Performance.gsheet\n",
            " Running_copy_binary.ipynb\n",
            " yelp_academic_dataset_review_50k.json\n",
            " yelp_academic_dataset_review.json\n",
            " yelp_academic_dataset_review_small.json\n",
            " yelp_academic_dataset_review_very_small.json\n",
            " Yelp_binary.ipynb\n",
            " Yelp_linear_regression.ipynb\n",
            " Yelp_ordinal_approach.ipynb\n",
            " Yelp_rescale.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWYA4_tKEq9G",
        "colab_type": "code",
        "outputId": "a3b78c34-1560-49d1-901a-81537ef6613b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Load data\n",
        "df = pd.read_json(path_or_buf='yelp_academic_dataset_review.json',lines=True)\n",
        "\n",
        "# Keep only reviews and score\n",
        "df = df[['stars','text']]\n",
        "\n",
        "# Print size of data\n",
        "print('Number of reviews before trimming:', len(df))\n",
        "\n",
        "# Limit size of data set (to avoid awkward ends)\n",
        "limit = 228000\n",
        "df = df.iloc[0:limit]\n",
        "print('Number of reviews after trimming:', len(df))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of reviews before trimming: 229907\n",
            "Number of reviews after trimming: 228000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcnxeJ7rKNoU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preprocess reviews\n",
        "def preprocess_text(sen):\n",
        "  #Make all lower case\n",
        "  sen = sen.lower()\n",
        "\n",
        "  # Remove digits\n",
        "  sen = re.sub(r'\\d+', '', sen)\n",
        "\n",
        "  # Remove punctuation\n",
        "  sen = sen.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "  # Remove newlines\n",
        "  sen = re.sub(r'\\n', ' ', sen)\n",
        "\n",
        "  return sen\n",
        "  \n",
        "X = []\n",
        "sentences = list(df['text'])\n",
        "for sen in sentences:\n",
        "    X.append(preprocess_text(sen))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8bdXMLaKNza",
        "colab_type": "code",
        "outputId": "7785c063-8ff9-4080-ef6d-6cff14952de9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "# Examine ratings and make into ordinal coding\n",
        "\n",
        "scores = np.array(df['stars'])\n",
        "\n",
        "# Plot distribution\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from matplotlib.ticker import PercentFormatter\n",
        "\n",
        "plt.hist(scores, weights=np.ones(len(scores)) / len(scores))\n",
        "plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
        "plt.xlabel(\"Number of stars\")\n",
        "plt.xticks((1,2,3,4,5) ,('1', '2', '3', '4', '5'))\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Ordinal targets - Example: 3 = [1, 1, 0, 0]\n",
        "def transform_targets(target):\n",
        "  target_array = np.zeros(4)\n",
        "  target_array[0:target-1] = 1\n",
        "\n",
        "  return target_array\n",
        "\n",
        "scores_list = []\n",
        "for target in scores:\n",
        "  scores_list.append(transform_targets(target))\n",
        "\n",
        "scores = np.array(scores_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEGCAYAAACHGfl5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coa9znkMBwp9",
        "colab_type": "code",
        "outputId": "50308567-6a3b-4025-f805-9848b995898f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Split data into train, test and validation\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, scores, test_size=0.20, random_state=42)\n",
        "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
        "\n",
        "print(\"Size of train data: {}\".format(len(X_train)))\n",
        "print(\"Size of validation data: {}\".format(len(X_val)))\n",
        "print(\"Size of test data: {}\".format(len(X_test)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of train data: 182400\n",
            "Size of validation data: 22800\n",
            "Size of test data: 22800\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXT2btTkCGP2",
        "colab_type": "code",
        "outputId": "a57129dc-a89f-499f-f4ee-bfb28436dd09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Tokenize reviews\n",
        "tokenizer = Tokenizer(num_words=20000)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "X_val = tokenizer.texts_to_sequences(X_val)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocab size is', vocab_size) # But we only keep then amount specified in num_words"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size is 185873\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oW03uDyCTyc_",
        "colab_type": "code",
        "outputId": "52bbe4f5-f03f-49df-e1f8-2989daee680f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        }
      },
      "source": [
        "# Check the lengths of reviews (in words)\n",
        "reviews_len = [len(x) for x in X_train]\n",
        "pd.Series(reviews_len).hist()\n",
        "plt.xlabel(\"Length of review\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()\n",
        "\n",
        "pd.Series(reviews_len).describe()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEGCAYAAACkQqisAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6s0QkST1ZohIknozRCRJvRkikqTe/j8S+GclVf+waQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    182400.00000\n",
              "mean        127.12006\n",
              "std         110.93852\n",
              "min           0.00000\n",
              "25%          52.00000\n",
              "50%          97.00000\n",
              "75%         168.00000\n",
              "max         990.00000\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqG78oegCRM1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Padding\n",
        "maxlen = 200\n",
        "\n",
        "X_train = pad_sequences(X_train, padding='pre', maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding='pre', maxlen=maxlen)\n",
        "X_val = pad_sequences(X_val, padding='pre', maxlen=maxlen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3P-eeVZVnDw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create Tensor datasets\n",
        "train_data = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
        "valid_data = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\n",
        "test_data = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
        "\n",
        "# Dataloaders\n",
        "batch_size = 50\n",
        "\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hm2StMcMV425",
        "colab_type": "code",
        "outputId": "0d5c124f-6edb-451a-abf4-2f62854cab75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Let's have a look at the input and target\n",
        "dataiter = iter(train_loader)\n",
        "sample_x, sample_y = dataiter.next()\n",
        "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
        "print('Sample input: \\n', sample_x)\n",
        "print()\n",
        "print('Sample label size: ', sample_y.size()) # batch_size\n",
        "print('Sample label: \\n', sample_y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample input size:  torch.Size([50, 200])\n",
            "Sample input: \n",
            " tensor([[   0,    0,    0,  ...,  256,   16,   28],\n",
            "        [   0,    0,    0,  ...,  160,  837, 7997],\n",
            "        [   0,    0,    0,  ...,   32,    4,  101],\n",
            "        ...,\n",
            "        [3990,   50,   21,  ...,    1,  331,  292],\n",
            "        [   0,    0,    0,  ...,  106,  318,   10],\n",
            "        [   0,    0,    0,  ...,  462, 2427, 4242]], dtype=torch.int32)\n",
            "\n",
            "Sample label size:  torch.Size([50, 4])\n",
            "Sample label: \n",
            " tensor([[1., 1., 1., 0.],\n",
            "        [1., 1., 1., 0.],\n",
            "        [1., 1., 0., 0.],\n",
            "        [1., 1., 0., 0.],\n",
            "        [1., 1., 1., 0.],\n",
            "        [1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [1., 1., 0., 0.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 0., 0.],\n",
            "        [1., 1., 1., 0.],\n",
            "        [1., 1., 0., 0.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [1., 1., 0., 0.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 0.],\n",
            "        [1., 1., 1., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 0., 0.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 0., 0.],\n",
            "        [1., 1., 0., 0.],\n",
            "        [1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [1., 0., 0., 0.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 0.],\n",
            "        [1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.]], dtype=torch.float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmZKrB9OV46m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define network\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # embedding and LSTM layers\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
        "\n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        \n",
        "        # linear and sigmoid layers\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        x = x.cuda()\n",
        "        \n",
        "        # embeddings and lstm_out\n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "    \n",
        "        # stack up lstm outputs\n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "        \n",
        "        # dropout and fully-connected layer\n",
        "        out = self.dropout(lstm_out)\n",
        "        out = self.fc(out)\n",
        "\n",
        "        # sigmoid function\n",
        "        sig_out = self.sig(out)\n",
        "        \n",
        "        # reshape to be batch_size first\n",
        "        sig_out = sig_out.view(batch_size,maxlen,-1)\n",
        "        \n",
        "        sig_out = sig_out[:, -1] # get last batch of labels\n",
        "        \n",
        "        # return last sigmoid output and hidden state\n",
        "        return sig_out, hidden\n",
        "\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if (train_on_gpu):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
        "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
        "        \n",
        "        return hidden\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWTPKikqY70c",
        "colab_type": "code",
        "outputId": "0ad4b7fe-02a9-4f90-811c-e94d6361189d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# Hyperparameters and initialization of model\n",
        "output_size = 4\n",
        "embedding_dim = 512\n",
        "hidden_dim = 512\n",
        "n_layers = 1\n",
        "net = Net(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "print(net)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (embedding): Embedding(185873, 512)\n",
            "  (lstm): LSTM(512, 512, batch_first=True)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=4, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDAQdTX6DXwG",
        "colab_type": "code",
        "outputId": "5872b00b-0893-40ad-c866-0e94acb8f62f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 742
        }
      },
      "source": [
        "# loss and optimization functions\n",
        "lr=0.0005\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=1e-5)\n",
        "\n",
        "# Add learning rate decay\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
        "\n",
        "# training params\n",
        "epochs = 10\n",
        "\n",
        "print_every = 1000\n",
        "clip=5 # gradient clipping\n",
        "\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "# move model to GPU, if available\n",
        "if(train_on_gpu):\n",
        "    net.cuda()\n",
        "\n",
        "# Track loss\n",
        "training_loss, validation_loss = [], []\n",
        "loss_list = []\n",
        "\n",
        "net.train()\n",
        "# train for some number of epochs\n",
        "for e in range(epochs):\n",
        "\n",
        "    # Track loss\n",
        "    epoch_training_loss = 0\n",
        "    epoch_validation_loss = 0\n",
        "\n",
        "    # For validation set\n",
        "    val_h = net.init_hidden(batch_size)\n",
        "    val_losses = []\n",
        "    net.eval()\n",
        "    for inputs, labels in valid_loader:\n",
        "\n",
        "        # Creating new variables for the hidden state\n",
        "        val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "        if(train_on_gpu):\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "        inputs = inputs.type(torch.LongTensor)\n",
        "        output, val_h = net(inputs, val_h)\n",
        "\n",
        "        val_loss = criterion(output.squeeze(), labels.float())\n",
        "\n",
        "        epoch_validation_loss += val_loss.detach().cpu().data.numpy()\n",
        "    \n",
        "    net.train()\n",
        "\n",
        "    # initialize hidden state\n",
        "    h = net.init_hidden(batch_size)\n",
        "\n",
        "    # For each review in training set\n",
        "    counter = 0\n",
        "    for inputs, labels in train_loader:\n",
        "        counter += 1\n",
        "        if counter % print_every == 0:\n",
        "            print(counter,\"Train\")\n",
        "\n",
        "        if(train_on_gpu):\n",
        "            inputs = inputs.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "        # Creating new variables for the hidden state\n",
        "        h = tuple([each.data for each in h])\n",
        "\n",
        "        # zero accumulated gradients\n",
        "        net.zero_grad()\n",
        "\n",
        "        # get the output from the model\n",
        "        inputs = inputs.type(torch.LongTensor)\n",
        "        output, h = net(inputs, h)\n",
        "\n",
        "        # calculate the loss and perform backprop\n",
        "        loss = criterion(output.squeeze(), labels.float())\n",
        "        loss.backward()\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem\n",
        "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        #Save loss\n",
        "        epoch_training_loss += loss.detach().cpu().data.numpy()\n",
        "\n",
        "    print('Epoch: ', e+1,)\n",
        "    print('training loss:', epoch_training_loss / len(train_data)) # Divided by number of batches\n",
        "    print('validation loss:', epoch_validation_loss / len(valid_data))\n",
        "\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Save loss for plot\n",
        "    training_loss.append(epoch_training_loss / len(train_data))\n",
        "    validation_loss.append(epoch_validation_loss / len(valid_data))\n",
        "\n",
        "# Plot training and validation loss of training and validation batches\n",
        "epoch = np.arange(len(training_loss))\n",
        "plt.figure()\n",
        "plt.plot(epoch, training_loss, 'r', label='Training loss',)\n",
        "plt.plot(epoch, validation_loss, 'b', label='Validation loss')\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch'), plt.ylabel('loss')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000 Train\n",
            "2000 Train\n",
            "3000 Train\n",
            "Epoch:  1\n",
            "training loss: 0.0021039061944752016\n",
            "validation loss: 0.00499663663015031\n",
            "1000 Train\n",
            "2000 Train\n",
            "3000 Train\n",
            "Epoch:  2\n",
            "training loss: 0.0016534176826060406\n",
            "validation loss: 0.0019006678213675818\n",
            "1000 Train\n",
            "2000 Train\n",
            "3000 Train\n",
            "Epoch:  3\n",
            "training loss: 0.0015771095702841289\n",
            "validation loss: 0.0016376004083768318\n",
            "1000 Train\n",
            "2000 Train\n",
            "3000 Train\n",
            "Epoch:  4\n",
            "training loss: 0.0015314625482802002\n",
            "validation loss: 0.0015600691998802255\n",
            "1000 Train\n",
            "2000 Train\n",
            "3000 Train\n",
            "Epoch:  5\n",
            "training loss: 0.00149938338854873\n",
            "validation loss: 0.0015610827801324297\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-d351113afde2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;31m#Save loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mepoch_training_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;31m#print('Epoch: ', e+1, 'with learning rate:', scheduler.get_lr())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nKv94OBYyXN",
        "colab_type": "code",
        "outputId": "39badcc7-7cb4-4274-aa3b-77e585ab6e16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Get test data loss and accuracy\n",
        "\n",
        "# Function to convert nominal coding to rationg\n",
        "def scan_output(array):\n",
        "  rating = 1\n",
        "  for i in range(len(array)):\n",
        "    if array[i] > 0.5:\n",
        "      rating += 1\n",
        "    else:\n",
        "      break\n",
        "  return rating\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "test_losses = [] # track loss\n",
        "pred_list = []\n",
        "target_list = []\n",
        "\n",
        "T = 0.5 # Cutoff for nominal coding\n",
        "\n",
        "# init hidden state\n",
        "h = net.init_hidden(batch_size)\n",
        "\n",
        "net.eval()\n",
        "# iterate over test data\n",
        "for inputs, labels in test_loader:\n",
        "\n",
        "    # Creating new variables for the hidden state\n",
        "    h = tuple([each.data for each in h])\n",
        "\n",
        "    if(train_on_gpu):\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "    \n",
        "    # get predicted outputs\n",
        "    inputs = inputs.type(torch.LongTensor)\n",
        "    output, h = net(inputs, h)\n",
        "    \n",
        "    # calculate loss\n",
        "    test_loss = criterion(output.squeeze(), labels.float())\n",
        "    test_losses.append(test_loss.item())\n",
        "    \n",
        "    # convert to rating\n",
        "    output_array = output.cpu().data.numpy()\n",
        "    pred = np.apply_along_axis(scan_output, 1, output_array)\n",
        "    pred_list = np.concatenate((pred_list, pred))\n",
        "\n",
        "    # compare predictions to true label\n",
        "    target_array = labels.cpu().numpy()\n",
        "    target = np.apply_along_axis(scan_output, 1, target_array)\n",
        "    target_list = np.concatenate((target_list, target))\n",
        "\n",
        "# accuracy over all test data\n",
        "from sklearn.metrics import accuracy_score\n",
        "test_acc = accuracy_score(target_list,pred_list)\n",
        "#test_acc = np.mean(accuracy)\n",
        "print(\"Test accuracy: {:.3f}\".format(test_acc))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.597\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}